{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Classification\n",
    "\n",
    "The goal of classification is to correctly assign a label to unseen instances of the input data. \n",
    "Here, we will introduce basic approaches to classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "For the first example, we will use the iris dataset.\n",
    "This set consists of 4 measurements on three classes or irises. \n",
    "The data contains 50 exemplars per class. The size of the entire dataset is 150 exemplars.\n",
    "This dataset is available with the sklearn library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import datasets\n",
    "\n",
    "# load the iris data set\n",
    "iris = datasets.load_iris()\n",
    "# extract input output pairs\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "Make a scatter plot using the first two dimensions (column of X) of the dataset.\n",
    "Color points by their label value (Y)\n",
    "\n",
    "You can use the function \"plt.scatter\"\n",
    "Hint: use ListedColormap to define the color for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write some lines of code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First classification strategy: Asign ta point the class of the nearest point in the training dataset\n",
    "This strategy is known as the 1-nearest neighbor classification rule (1-NN). To implement this strategy, we need to define a distance between input vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclideanDistance(x1, x2):\n",
    "    d2 = np.sum((x1 - x2)**2, axis=1)\n",
    "    return np.sqrt(d2)\n",
    "\n",
    "class oneNNClassifier(object):\n",
    "    def __init__(self, X_train, Y_train, dist=euclideanDistance):\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.dist = dist\n",
    "    \n",
    "    def classify(self, X):\n",
    "        Y = np.zeros(X.shape[0], dtype=np.int)\n",
    "        for i, Xi in enumerate(X):\n",
    "            NNidx = np.argmin(self.dist(Xi, self.X_train))\n",
    "            Y[i] = self.Y_train[NNidx]\n",
    "        return Y\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the classifier on the train data\n",
    "my1NN = oneNNClassifier(X, Y)\n",
    "Y_test = my1NN.classify(X)\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "Extend the 1-NN classifier to k-NN. We will use majority vote to decide the class of the test sample.\n",
    "\n",
    "Hint: You can use np.argpartition(A, k) to find the indexes of the k elements of the list with thesmalles values, and np.unique to implement the majority vote rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write some lines of code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the classifer on the train data\n",
    "mykNN = kNNClassifier(X, Y)\n",
    "Y_test = mykNN.classify(X,5)\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classification using softmax\n",
    "In this exercise, we implemneted the kNN classifier to gain intuition about classification problem and create a baseline.\n",
    "For linear classification, we will use the functions available in scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "mySoftmax = LogisticRegression()\n",
    "mySoftmax.fit(X, Y)\n",
    "Y_test = mySoftmax.predict(X)\n",
    "print (Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "Compare the partitions of 1-NN , k-NN for k=[3,5,8], and the softmax logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write some lines of code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
